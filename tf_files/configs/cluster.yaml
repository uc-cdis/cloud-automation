# Unique name of Kubernetes cluster. In order to deploy
# more than one cluster into the same AWS account, this
# name must not conflict with an existing cluster.
clusterName: ${cluster_name}

# DNS name routable to the Kubernetes controller nodes
# from worker nodes and external clients. Configure the options
# below if you'd like kube-aws to create a Route53 record sets/hosted zones
# for you.  Otherwise the deployer is responsible for making this name routable
externalDNSName: controller.internal.io

keyName: ${key_name}
sshAuthorizedKeys:
${kube_additional_keys}

region: "${aws_region}"

kmsKeyArn: "${kms_key}"

addons:
  clusterAutoscaler:
    enabled: true

worker:
  nodePools:
    - # Name of this node pool. Must be unique among all the node pools in this cluster
      name: nodepool
      autoscaling:
        clusterAutoscaler:
          enabled: true
      subnets:
        - name: private

controller:
  subnets:
  - name: private
  loadBalancer:
    private: true

etcd:
  dataVolume:
    encrypted: true
  subnets:
  - name: private

routeTableId: ${route_table_id}
# CIDR for Kubernetes VPC. If vpcId is specified, must match the CIDR of existing vpc.
# vpcCIDR: "10.0.0.0/16"
vpc:
  id: ${vpc_id}

# CIDR for Kubernetes subnet when placing nodes in a single availability zone (not highly-available) Leave commented out for multi availability zone setting and use the below `subnets` section instead.
# instanceCIDR: "10.0.0.0/24"
subnets:
  - name: private
    #private: true
    id: "${subnet_id}"
    instanceCIDR: "${subnet_cidr}"
    availabilityZone: "${subnet_zone}"
    securityGroupIds:
      - ${security_group_id}

mapPublicIPs: false

stackTags:
  Environment: ${cluster_name}
  Organization: "Basic Service"
hostedZoneId: ${hosted_zone}
createRecordSet: true
