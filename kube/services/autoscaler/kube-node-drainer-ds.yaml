kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: kube-node-drainer-ds
  namespace: kube-system
  labels:
    k8s-app: kube-node-drainer-ds
spec:
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        k8s-app: kube-node-drainer-ds
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      #priorityClassName: system-node-critical
      tolerations:
      # for the case of jupyter nodepool, we don't care if the drainer is running on the workers 
      #- operator: Exists
      #  effect: NoSchedule
      - operator: Exists
        effect: NoExecute
      - operator: Exists
        key: CriticalAddonsOnly
      initContainers:
        - name: hyperkube
          image: k8s.gcr.io/hyperkube-amd64:K8S_VERSION
          command:
          - /bin/cp
          - -f
          - /hyperkube
          - /workdir/hyperkube
          volumeMounts:
          - mountPath: /workdir
            name: workdir
      containers:
        - name: kube-node-drainer
          image: quay.io/coreos/awscli:master
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          command:
          - /bin/sh
          - -xec
          - |
            metadata() { curl -s -S -f http://169.254.169.254/2016-09-02/"$1"; }
            asg()      { aws --region="${REGION}" autoscaling "$@"; }

            # Hyperkube binary is not statically linked, so we need to use
            # the musl interpreter to be able to run it in this image
            # See: https://github.com/kubernetes-incubator/kube-aws/pull/674#discussion_r118889687
            kubectl() { /lib/ld-musl-x86_64.so.1 /opt/bin/hyperkube kubectl "$@"; }

            INSTANCE_ID=$(metadata meta-data/instance-id)
            REGION=$(metadata dynamic/instance-identity/document | jq -r .region)
            [ -n "${REGION}" ]

            # Not customizable, for now
            POLL_INTERVAL=10

            # Used to identify the source which requested the instance termination
            termination_source=''

            # Instance termination detection loop
            while sleep ${POLL_INTERVAL}; do

              # Spot instance termination check
              http_status=$(curl -o /dev/null -w '%{http_code}' -sL http://169.254.169.254/latest/meta-data/spot/termination-time)
              if [ "${http_status}" -eq 200 ]; then
                termination_source=spot
                break
              fi

              # Termination ConfigMap check
              if [ -e /etc/kube-node-drainer/asg ] && grep -q "${INSTANCE_ID}" /etc/kube-node-drainer/asg; then
                termination_source=asg
                break
              fi
            done

            # Node draining loop
            while true; do
              echo Node is terminating, draining it...

              if ! kubectl drain --ignore-daemonsets=true --delete-local-data=true --force=true --timeout=60s "${NODE_NAME}"; then
                echo Not all pods on this host can be evicted, will try again
                continue
              fi
              echo All evictable pods are gone

              if [ "${termination_source}" == asg ]; then
                echo Notifying AutoScalingGroup that instance ${INSTANCE_ID} can be shutdown
                ASG_NAME=$(asg describe-auto-scaling-instances --instance-ids "${INSTANCE_ID}" | jq -r '.AutoScalingInstances[].AutoScalingGroupName')
                HOOK_NAME=$(asg describe-lifecycle-hooks --auto-scaling-group-name "${ASG_NAME}" | jq -r '.LifecycleHooks[].LifecycleHookName' | grep -i nodedrainer)
                asg complete-lifecycle-action --lifecycle-action-result CONTINUE --instance-id "${INSTANCE_ID}" --lifecycle-hook-name "${HOOK_NAME}" --auto-scaling-group-name "${ASG_NAME}"
              fi

              # Expect instance will be shut down in 5 minutes
              sleep 300
            done
          volumeMounts:
          - mountPath: /opt/bin
            name: workdir
          - mountPath: /etc/kube-node-drainer
            name: kube-node-drainer-status
            readOnly: true
      volumes:
      - name: workdir
        emptyDir: {}
      - name: kube-node-drainer-status
        projected:
          sources:
          - configMap:
              name: kube-node-drainer-status
              optional: true
