# gen3 job run etl ETL_FORCED TRUE
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    karpenter.sh/do-not-evict: "true"
  name: etl
spec:
  backoffLimit: 0
  template:
    metadata:
      labels:
        app: gen3job
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: karpenter.sh/capacity-type
                operator: In
                values:
                - on-demand
          - weight: 99
            preference:
              matchExpressions:
              - key: eks.amazonaws.com/capacityType
                operator: In
                values:
                - ONDEMAND
      volumes:
      - name: signal-volume
        emptyDir: {}
      - name: creds-volume
        secret:
          secretName: "peregrine-creds"
      - name: etl-mapping
        configMap:
          name: etl-mapping
      - name: fence-yaml
        configMap:
          name: fence
      containers:
      - name: gen3-spark
        GEN3_SPARK_IMAGE
        ports:
        - containerPort: 22
        - containerPort: 9000
        - containerPort: 8030
        - containerPort: 8031
        - containerPort: 8032
        - containerPort: 7077
        readinessProbe:
          tcpSocket:
            port: 9000
          initialDelaySeconds: 10
          periodSeconds: 30
        env:
        - name: DICTIONARY_URL
          valueFrom:
            configMapKeyRef:
              name: manifest-global
              key: dictionary_url
        - name: HADOOP_URL
          value: hdfs://0.0.0.0:9000
        - name: HADOOP_HOST
          value: 0.0.0.0
        volumeMounts:
          - mountPath: /usr/share/pod
            name: signal-volume
            readOnly: true
        imagePullPolicy: Always
        resources:
          requests:
            cpu: 3
            memory: 4Gi
        command: ["/bin/bash" ]
        args: 
          - "-c"
          - |
            trap 'exit 0' SIGINT SIGQUIT SIGTERM
            # get /usr/local/share/ca-certificates/cdis-ca.crt into system bundle
            ssh server sudo /etc/init.d/ssh start
            # update-ca-certificates
            python run_config.py
            hdfs namenode -format
            hdfs --daemon start namenode
            hdfs --daemon start datanode
            yarn --daemon start resourcemanager
            yarn --daemon start nodemanager
            hdfs dfsadmin -safemode leave
            hdfs dfs -mkdir /result
            hdfs dfs -mkdir /jars
            hdfs dfs -mkdir /archive
            /spark/sbin/start-all.sh
            while true; do sleep 5; done
      - name: tube
        imagePullPolicy: Always
        GEN3_TUBE_IMAGE
        ports:
        - containerPort: 80
        env:
        - name: DICTIONARY_URL
          valueFrom:
            configMapKeyRef:
              name: manifest-global
              key: dictionary_url
        - name: HADOOP_URL
          value: hdfs://spark-service:9000
        - name: ES_URL
          value: esproxy-service
        - name: HADOOP_HOST
          value: spark-service
        - name: HADOOP_CLIENT_OPTS
          value: -Xmx1g
        - name: SPARK_EXECUTOR_MEMORY
          value: 4g
        - name: SPARK_DRIVER_MEMORY
          value: 6g
        - name: ETL_FORCED
          GEN3_ETL_FORCED|-value: "TRUE"-|
        - name: gen3Env
          valueFrom:
            configMapKeyRef:
              name: manifest-global
              key: hostname
        - name: slackWebHook
          valueFrom:
            configMapKeyRef:
              name: global
              key: slack_webhook
              optional: true
        volumeMounts:
            # Volume to signal when to kill spark
          - mountPath: /usr/share/pod
            name: signal-volume
          - name: "creds-volume"
            readOnly: true
            mountPath: "/gen3/tube/creds.json"
            subPath: creds.json
          - name: "etl-mapping"
            readOnly: true
            mountPath: "/gen3/tube/etlMapping.yaml"
            subPath: "etlMapping.yaml"
          - name: "fence-yaml"
            readOnly: true
            mountPath: "/gen3/tube/user.yaml"
            subPath: user.yaml
        resources:
          limits:
            cpu: 2
            memory: 10Gi
          requests:
            cpu: 2
        command: ["/bin/bash" ]
        args:
          - "-c"
          - |
            echo 'options use-vc' >> /etc/resolv.conf
            if [[ $ETL_FORCED != "false" ]]; then
              python run_config.py && python run_etl.py --force
            else
              python run_config.py && python run_etl.py
            fi
            exitcode=$?
            if [[ "${slackWebHook}" != 'None' ]]; then
              if [[ $exitcode == 1 ]]; then
                curl -X POST --data-urlencode "payload={\"text\": \"JOBFAIL: ETL job on ${gen3Env}\"}" "${slackWebHook}"
              else
                curl -X POST --data-urlencode "payload={\"text\": \"SUCCESS: ETL job on ${gen3Env}\"}" "${slackWebHook}"
              fi
            fi
            echo "Exit code: $exitcode"
            exit "$exitcode"
      restartPolicy: Never
