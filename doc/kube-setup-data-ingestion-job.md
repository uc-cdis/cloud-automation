The data-ingestion-job is specific to the BioData Catalyst project.

To prep:

Specify an image for the pipeline in the `versions` block of your manifest, for example:
`"data-ingestion-pipeline": "quay.io/cdis/bdcat-data-ingestion"`

Fill out the config file (see below for example format) with creds here: `$(gen3_secrets_folder)/g3auto/data-ingestion-job/data_ingestion_job_config.json`

Place a newline-separated list of phs IDs here: `$(gen3_secrets_folder)/g3auto/data-ingestion-job/phsids.txt`

Optionally place a "data\_requiring\_manual\_review.tsv" file here: `$(gen3_secrets_folder)/g3auto/data-ingestion-job/data_requiring_manual_review.tsv`

If `CREATE_GENOME_MANIFEST` is false, the genome file manifest is required to live in a "genome\_file\_manifest" here: `$(gen3_secrets_folder)/g3auto/data-ingestion-job/genome_file_manifest.csv`

Usage:
`gen3 kube-setup-data-ingestion-job CREATE_GOOGLE_GROUPS <bool> CREATE_GENOME_MANIFEST <bool>`

The Dockerfile executable that this job runs can be found in this repository: `https://github.com/uc-cdis/bdcat-data-ingestion`

If the executable is run successfully, a new pull request will be created with the job outputs in the repository specified in the config file below. (Likely `https://github.com/uc-cdis/dataSTAGE-data-ingestion-private`)

Config file example, with some fields pre-filled:

    {
    "gs_creds": {
        "type": "service_account",
        "project_id": "",
        "private_key_id": "",
        "private_key": "",
        "client_email": "",
        "client_id": "",
        "auth_uri": "https://accounts.google.com/o/oauth2/auth",
        "token_uri": "https://oauth2.googleapis.com/token",
        "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
        "client_x509_cert_url": ""
    },
    "genome_bucket_aws_creds": {
        "aws_access_key_id": "",
        "aws_secret_access_key": ""
    },
    "local_data_aws_creds": {
        "bucket_name": "",
        "aws_access_key_id": "",
        "aws_secret_access_key": ""
    },
    "gcp_project_id": "",
    "github_user_email": "",
    "github_personal_access_token": "",
    "github_user_name": "",
    "git_org_to_pr_to": "uc-cdis",
    "git_repo_to_pr_to": "dataSTAGE-data-ingestion-private"
    }

`gs_creds`: This block can be autogenerated by following the directions [here](https://cloud.google.com/iam/docs/creating-managing-service-account-keys). These credentials are for the Google Cloud bucket used to generate a genome file manifest. 

`genome_bucket_aws_creds`: These credentials are for the AWS bucket used to generate a genome file manifest. 

`local_data_aws_creds`: These credentials are for the AWS bucket optionally used to store inputs from the user. If the user opts to provide a pre-created genome file manifest and sets CREATE\_GENOME\_MANIFEST to false, the user's file will be placed in this s3 bucket for easy retrieval by the pipeline image.

`gcp_project_id`: This argument is also related to the generate-file-manifest.sh script which generates a genome file manifest. The field is set to a GCP_PROJECT_ID environment variable which allows access to a requester pays bucket.

`github_user_email`: This is the email of the user that the job will log in as in order to make a pull request with the outputs of the job.

`github_personal_access_token`: This is a generated personal access token for the user that the job will log in as in order to make a pull request with the outputs of the job. Read and write privileges on the repository in question are required.

`github_user_name`: This is the username of the account that the job will log in as in order to make a pull request with the outputs of the job.

`git_org_to_pr_to`: This is the organization that owns the git repository that the outputs of the job will be PR'd to.

`git_repo_to_pr_to`: This is the git repository that the outputs of the job will be PR'd to.
